{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Menon-Vineet/Python-Codes/blob/main/ims_bearing_anomaly_detection_refactored.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0358eacf",
      "metadata": {
        "id": "0358eacf"
      },
      "source": [
        "# Unsupervised Bearing Anomaly Detection (IMS / NASA PCoE)\n",
        "\n",
        "**Author:** Vineet Menon (el jefe)  \n",
        "**Date:** 2026-01-19  \n",
        "**Goal:** I compare classical unsupervised anomaly detectors (via **PyCaret**) against a lightweight **BiLSTM Autoencoder** on the IMS run-to-failure bearing vibration dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## What you get in this notebook\n",
        "\n",
        "- A **Colab-ready** workflow: install → download → featurize → train → score → visualize\n",
        "- Self-contained utilities (no missing local scripts)\n",
        "- Reproducible runs (seeded) and clean, readable code with docstrings + type hints\n",
        "\n",
        "> Dataset download source: NASA Prognostics Center of Excellence (PCoE) “Bearings” ZIP on PHM S3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8ad7b09",
      "metadata": {
        "id": "e8ad7b09"
      },
      "source": [
        "## 0. Environment setup (Colab)\n",
        "\n",
        "If you run this in Google Colab, execute the next cell once to install dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ba4ee60",
      "metadata": {
        "id": "9ba4ee60"
      },
      "outputs": [],
      "source": [
        "# I keep installs in one place so the notebook is easy to share and re-run on Colab.\n",
        "# Note: PyCaret pulls a sizable dependency tree; first install may take a bit.\n",
        "\n",
        "%%capture\n",
        "!pip -q install -U \"pycaret[full]\" pandas numpy matplotlib scikit-learn tqdm joblib\n",
        "!pip -q install -U torch --index-url https://download.pytorch.org/whl/cpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "783f8137",
      "metadata": {
        "id": "783f8137"
      },
      "source": [
        "## 1. Imports and global configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39a0b64a",
      "metadata": {
        "id": "39a0b64a"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# PyCaret (anomaly)\n",
        "from pycaret.anomaly import setup as anomaly_setup\n",
        "from pycaret.anomaly import create_model, predict_model, save_model\n",
        "\n",
        "# Torch (BiLSTM autoencoder)\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# I set seeds so results are repeatable across runs (as much as possible for CPU training).\n",
        "SEED: int = 1234\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# I store all artifacts under one project folder so Colab + GitHub runs remain tidy.\n",
        "PROJECT_DIR = Path.cwd() / \"ims_anomaly_project\"\n",
        "DATA_DIR = PROJECT_DIR / \"data\"\n",
        "ARTIFACTS_DIR = PROJECT_DIR / \"artifacts\"\n",
        "PLOTS_DIR = PROJECT_DIR / \"plots\"\n",
        "\n",
        "for d in (DATA_DIR, ARTIFACTS_DIR, PLOTS_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Project dir:\", PROJECT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c26b72d2",
      "metadata": {
        "id": "c26b72d2"
      },
      "source": [
        "## 2. Utility functions\n",
        "\n",
        "I keep the notebook self-contained by implementing the dataset download, parsing, feature extraction, and plotting helpers directly here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4627378",
      "metadata": {
        "id": "a4627378"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "\n",
        "NASA_BEARINGS_ZIP_URL: str = \"https://phm-datasets.s3.amazonaws.com/NASA/4.+Bearings.zip\"\n",
        "\n",
        "def download_file(url: str, dst_path: Path, overwrite: bool = False) -> Path:\n",
        "    \"\"\"Download a file to a local path.\n",
        "\n",
        "    Args:\n",
        "        url: Public URL to download.\n",
        "        dst_path: Where I want to store the file.\n",
        "        overwrite: If True, I re-download even if the file exists.\n",
        "\n",
        "    Returns:\n",
        "        The path to the downloaded file.\n",
        "    \"\"\"\n",
        "    dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if dst_path.exists() and not overwrite:\n",
        "        print(f\"I am reusing existing file: {dst_path}\")\n",
        "        return dst_path\n",
        "\n",
        "    # I use urllib from the standard library to keep this notebook dependency-light.\n",
        "    import urllib.request\n",
        "\n",
        "    print(f\"I am downloading dataset to: {dst_path}\")\n",
        "    urllib.request.urlretrieve(url, dst_path)\n",
        "    return dst_path\n",
        "\n",
        "\n",
        "def unzip(zip_path: Path, extract_to: Path, overwrite: bool = False) -> Path:\n",
        "    \"\"\"Extract a ZIP archive.\n",
        "\n",
        "    Args:\n",
        "        zip_path: Path to a .zip file.\n",
        "        extract_to: Directory to extract into.\n",
        "        overwrite: If True, I delete the existing folder first.\n",
        "\n",
        "    Returns:\n",
        "        The extraction directory.\n",
        "    \"\"\"\n",
        "    if extract_to.exists() and overwrite:\n",
        "        shutil.rmtree(extract_to)\n",
        "\n",
        "    extract_to.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # I explicitly open and extract to avoid shelling out and to keep it cross-platform.\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        zf.extractall(extract_to)\n",
        "\n",
        "    return extract_to\n",
        "\n",
        "\n",
        "def parse_ims_filename_to_datetime(filename: str) -> datetime:\n",
        "    \"\"\"Parse IMS file timestamps like '2003.10.22.12.06.24' into a datetime.\"\"\"\n",
        "    stem = Path(filename).stem\n",
        "    return datetime.strptime(stem, \"%Y.%m.%d.%H.%M.%S\")\n",
        "\n",
        "\n",
        "def find_test_folder(root: Path, test_name: str) -> Path:\n",
        "    \"\"\"Locate a test folder (e.g., '1st_test', '2nd_test', '3rd_test') inside the extracted dataset.\"\"\"\n",
        "    candidates = list(root.rglob(test_name))\n",
        "    if not candidates:\n",
        "        raise FileNotFoundError(f\"I could not find '{test_name}' under {root}\")\n",
        "    return candidates[0]\n",
        "\n",
        "\n",
        "def load_ims_timeseries(\n",
        "    test_folder: Path,\n",
        "    bearing_col: int = 0,\n",
        "    max_files: Optional[int] = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Load raw IMS vibration files into a time-indexed dataframe.\n",
        "\n",
        "    Important:\n",
        "        Each file is a 1-second vibration snapshot sampled at high frequency.\n",
        "        Loading *every sample* can be huge, so I typically convert each file into one compact feature row.\n",
        "\n",
        "    Args:\n",
        "        test_folder: Folder containing timestamp-named vibration files.\n",
        "        bearing_col: Which column/channel I load (0-based).\n",
        "        max_files: If set, I only load the first N files (useful for quick demos).\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with a single column 'signal' (numpy arrays) indexed by timestamp.\n",
        "    \"\"\"\n",
        "    files = sorted([p for p in test_folder.iterdir() if p.is_file()])\n",
        "    if max_files is not None:\n",
        "        files = files[:max_files]\n",
        "\n",
        "    rows: List[pd.Series] = []\n",
        "    idx: List[pd.Timestamp] = []\n",
        "\n",
        "    for fp in tqdm(files, desc=f\"Loading raw signal col={bearing_col}\"):\n",
        "        ts = parse_ims_filename_to_datetime(fp.name)\n",
        "        try:\n",
        "            arr = pd.read_csv(fp, header=None, delim_whitespace=True).iloc[:, bearing_col].to_numpy(dtype=float)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"I failed reading file {fp}: {e}\") from e\n",
        "\n",
        "        # I store the entire 1-second snapshot as a numpy array object; downstream I will featurize it.\n",
        "        rows.append(pd.Series({\"signal\": arr}))\n",
        "        idx.append(pd.Timestamp(ts))\n",
        "\n",
        "    df = pd.DataFrame(rows, index=idx).sort_index()\n",
        "    df.index.name = \"timestamp\"\n",
        "    return df\n",
        "\n",
        "\n",
        "def rms(x: np.ndarray) -> float:\n",
        "    \"\"\"Root-mean-square, a standard vibration energy feature.\"\"\"\n",
        "    return float(np.sqrt(np.mean(np.square(x), dtype=float)))\n",
        "\n",
        "\n",
        "def peak_to_peak(x: np.ndarray) -> float:\n",
        "    \"\"\"Peak-to-peak amplitude.\"\"\"\n",
        "    return float(np.max(x) - np.min(x))\n",
        "\n",
        "\n",
        "def kurtosis_excess(x: np.ndarray) -> float:\n",
        "    \"\"\"Excess kurtosis (kurtosis - 3) without requiring scipy.\"\"\"\n",
        "    x = x.astype(float)\n",
        "    mu = x.mean()\n",
        "    s2 = np.mean((x - mu) ** 2)\n",
        "    if s2 == 0:\n",
        "        return 0.0\n",
        "    m4 = np.mean((x - mu) ** 4)\n",
        "    return float(m4 / (s2 ** 2) - 3.0)\n",
        "\n",
        "\n",
        "def featurize_snapshot(signal: np.ndarray) -> Dict[str, float]:\n",
        "    \"\"\"Convert one raw vibration snapshot into a compact set of statistical features.\"\"\"\n",
        "    # I keep features simple, fast, and interpretable.\n",
        "    return {\n",
        "        \"rms\": rms(signal),\n",
        "        \"mean\": float(np.mean(signal)),\n",
        "        \"std\": float(np.std(signal)),\n",
        "        \"p2p\": peak_to_peak(signal),\n",
        "        \"kurtosis_excess\": kurtosis_excess(signal),\n",
        "    }\n",
        "\n",
        "\n",
        "def featurize_timeseries(raw_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Featurize a dataframe produced by `load_ims_timeseries` into numeric columns.\"\"\"\n",
        "    feats = [featurize_snapshot(sig) for sig in tqdm(raw_df[\"signal\"].to_list(), desc=\"Featurizing snapshots\")]\n",
        "    return pd.DataFrame(feats, index=raw_df.index)\n",
        "\n",
        "\n",
        "def plot_series(df: pd.DataFrame, columns: List[str], title: str, fname: str) -> None:\n",
        "    \"\"\"Plot time series columns and save under the project plots folder.\"\"\"\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    for col in columns:\n",
        "        plt.plot(df.index, df[col], label=col)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"timestamp\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out = PLOTS_DIR / f\"{fname}.png\"\n",
        "    plt.savefig(out, dpi=150)\n",
        "    plt.show()\n",
        "    print(\"Saved:\", out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ac6fb2",
      "metadata": {
        "id": "f4ac6fb2"
      },
      "source": [
        "## 3. Download + extract the dataset\n",
        "\n",
        "The IMS bearing dataset ships as a ZIP containing folders like `1st_test`, `2nd_test`, and `3rd_test`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa445bf",
      "metadata": {
        "id": "0aa445bf"
      },
      "outputs": [],
      "source": [
        "zip_path = DATA_DIR / \"NASA_4_Bearings.zip\"\n",
        "extract_root = DATA_DIR / \"NASA_4_Bearings\"\n",
        "\n",
        "# I download once and reuse locally (handy for iterative work).\n",
        "download_file(NASA_BEARINGS_ZIP_URL, zip_path, overwrite=False)\n",
        "unzip(zip_path, extract_root, overwrite=False)\n",
        "\n",
        "print(\"Extracted to:\", extract_root)\n",
        "print(\"Top-level contents:\", [p.name for p in extract_root.iterdir()][:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b3b1ec5",
      "metadata": {
        "id": "8b3b1ec5"
      },
      "source": [
        "## 4. Load one test run and build features\n",
        "\n",
        "To keep this notebook runnable on modest machines, I show a fast path:\n",
        "- Load **one channel** from a test folder\n",
        "- Convert each 1-second snapshot into a **feature row** (RMS, p2p, etc.)\n",
        "\n",
        "You can scale to more channels/features once the pipeline is working.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29d88c88",
      "metadata": {
        "id": "29d88c88"
      },
      "outputs": [],
      "source": [
        "TEST_NAME = \"2nd_test\"        # try: \"1st_test\", \"2nd_test\", \"3rd_test\"\n",
        "BEARING_COL = 0                 # vibration channel column index\n",
        "MAX_FILES = 800                 # I cap files for a quick demo; set None for full run (slower)\n",
        "\n",
        "test_folder = find_test_folder(extract_root, TEST_NAME)\n",
        "print(\"Using test folder:\", test_folder)\n",
        "\n",
        "raw_df = load_ims_timeseries(test_folder, bearing_col=BEARING_COL, max_files=MAX_FILES)\n",
        "feat_df = featurize_timeseries(raw_df)\n",
        "\n",
        "print(\"Feature dataframe shape:\", feat_df.shape)\n",
        "feat_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37955deb",
      "metadata": {
        "id": "37955deb"
      },
      "source": [
        "## 5. Quick visualization\n",
        "\n",
        "RMS is often a good first “health indicator” proxy for bearings (it tends to rise as damage accumulates).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f63e52a7",
      "metadata": {
        "id": "f63e52a7"
      },
      "outputs": [],
      "source": [
        "plot_series(\n",
        "    feat_df,\n",
        "    [\"rms\"],\n",
        "    title=f\"{TEST_NAME} - RMS over time (channel {BEARING_COL})\",\n",
        "    fname=\"rms_over_time\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac21f070",
      "metadata": {
        "id": "ac21f070"
      },
      "source": [
        "## 6. Train/test split (time-aware)\n",
        "\n",
        "Because this is *run-to-failure*, I treat the early segment as “mostly normal” and score later segments as potential anomalies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f105ac71",
      "metadata": {
        "id": "f105ac71"
      },
      "outputs": [],
      "source": [
        "TRAIN_FRACTION = 0.6\n",
        "\n",
        "n = len(feat_df)\n",
        "cut = int(n * TRAIN_FRACTION)\n",
        "\n",
        "train_df = feat_df.iloc[:cut].copy()\n",
        "test_df = feat_df.iloc[cut:].copy()\n",
        "\n",
        "print(\"Train rows:\", len(train_df), \"| Test rows:\", len(test_df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55bdf652",
      "metadata": {
        "id": "55bdf652"
      },
      "source": [
        "## 7. PyCaret anomaly models\n",
        "\n",
        "I use PyCaret to quickly compare multiple unsupervised detectors.\n",
        "\n",
        "Notes:\n",
        "- PyCaret expects purely numeric columns (our feature set is numeric by design)\n",
        "- I use the **train segment** to fit the model, then score the tail segment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe99885",
      "metadata": {
        "id": "dbe99885"
      },
      "outputs": [],
      "source": [
        "_ = anomaly_setup(\n",
        "    data=train_df,\n",
        "    session_id=SEED,\n",
        "    normalize=True,\n",
        "    silent=True,\n",
        "    html=False,\n",
        ")\n",
        "\n",
        "ANOMALY_MODELS: List[str] = [\"iforest\", \"knn\", \"lof\", \"svm\", \"mcd\"]\n",
        "pycaret_results: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "for model_name in ANOMALY_MODELS:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"I am training PyCaret anomaly model: {model_name}\")\n",
        "\n",
        "    model = create_model(model_name)\n",
        "    scored = predict_model(model, data=test_df)\n",
        "\n",
        "    # I persist the pipeline so the notebook is shareable and reusable.\n",
        "    save_model(model, str(ARTIFACTS_DIR / f\"{model_name}_pipeline\"))\n",
        "\n",
        "    pycaret_results[model_name] = scored\n",
        "\n",
        "print(\"\\nSaved model pipelines under:\", ARTIFACTS_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba655fce",
      "metadata": {
        "id": "ba655fce"
      },
      "source": [
        "### 7.1 Visualize anomaly scores (example)\n",
        "\n",
        "PyCaret returns:\n",
        "- `Anomaly` (0/1 flag)\n",
        "- `Anomaly_Score` (higher usually means “more anomalous”, model-dependent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d40f11c",
      "metadata": {
        "id": "3d40f11c"
      },
      "outputs": [],
      "source": [
        "MODEL_TO_PLOT = \"iforest\"\n",
        "\n",
        "scored = pycaret_results[MODEL_TO_PLOT].copy()\n",
        "print(\"Columns:\", list(scored.columns))\n",
        "\n",
        "plot_df = scored.join(test_df[[\"rms\"]], how=\"left\")\n",
        "\n",
        "plot_series(\n",
        "    plot_df,\n",
        "    [\"rms\", \"Anomaly_Score\"],\n",
        "    title=f\"{MODEL_TO_PLOT} - RMS vs Anomaly_Score\",\n",
        "    fname=f\"{MODEL_TO_PLOT}_score_vs_rms\",\n",
        ")\n",
        "\n",
        "anom_rate = float(plot_df[\"Anomaly\"].mean() * 100)\n",
        "print(f\"Anomaly rate (flagged=1) for {MODEL_TO_PLOT}: {anom_rate:.2f}%\")\n",
        "\n",
        "plot_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a67ed5ea",
      "metadata": {
        "id": "a67ed5ea"
      },
      "source": [
        "## 8. BiLSTM Autoencoder (PyTorch)\n",
        "\n",
        "Why this model:\n",
        "- Sequence models can learn temporal structure across time\n",
        "- Autoencoders give a natural **reconstruction error** as an anomaly score\n",
        "\n",
        "Here, I use a compact BiLSTM autoencoder on **scaled feature vectors**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccf95eba",
      "metadata": {
        "id": "ccf95eba"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class SequenceConfig:\n",
        "    \"\"\"Configuration for creating sliding windows.\"\"\"\n",
        "\n",
        "    window: int = 32\n",
        "    stride: int = 1\n",
        "\n",
        "\n",
        "def make_windows(x: np.ndarray, cfg: SequenceConfig) -> np.ndarray:\n",
        "    \"\"\"Create overlapping windows from a 2D array (time, features).\n",
        "\n",
        "    Args:\n",
        "        x: Array of shape (T, F).\n",
        "        cfg: Window/stride configuration.\n",
        "\n",
        "    Returns:\n",
        "        Windows of shape (N, window, F).\n",
        "    \"\"\"\n",
        "    if x.ndim != 2:\n",
        "        raise ValueError(\"I expected x to be 2D: (time, features).\")\n",
        "\n",
        "    T, _ = x.shape\n",
        "    if T < cfg.window:\n",
        "        raise ValueError(f\"I need at least {cfg.window} rows, but got {T}.\")\n",
        "\n",
        "    windows = []\n",
        "    for start in range(0, T - cfg.window + 1, cfg.stride):\n",
        "        windows.append(x[start : start + cfg.window])\n",
        "\n",
        "    return np.stack(windows, axis=0)\n",
        "\n",
        "\n",
        "class BiLSTMAutoencoder(nn.Module):\n",
        "    \"\"\"A small BiLSTM autoencoder for sequence reconstruction.\"\"\"\n",
        "\n",
        "    def __init__(self, n_features: int, hidden: int = 32) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(\n",
        "            input_size=n_features,\n",
        "            hidden_size=hidden,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=2 * hidden,\n",
        "            hidden_size=hidden,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "        self.proj = nn.Linear(2 * hidden, n_features)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # I encode the input sequence, then decode it back to the original feature dimension.\n",
        "        z, _ = self.encoder(x)\n",
        "        y, _ = self.decoder(z)\n",
        "        return self.proj(y)\n",
        "\n",
        "\n",
        "def train_autoencoder(\n",
        "    model: nn.Module,\n",
        "    x_train: np.ndarray,\n",
        "    epochs: int = 8,\n",
        "    batch_size: int = 64,\n",
        "    lr: float = 1e-3,\n",
        ") -> List[float]:\n",
        "    \"\"\"Train the autoencoder and return epoch losses.\"\"\"\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.SmoothL1Loss()  # Huber-like loss; robust to spikes.\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(torch.tensor(x_train, dtype=torch.float32))\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    epoch_losses: List[float] = []\n",
        "    for ep in range(1, epochs + 1):\n",
        "        losses = []\n",
        "        for (xb,) in loader:\n",
        "            xb = xb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            recon = model(xb)\n",
        "            loss = loss_fn(recon, xb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            losses.append(float(loss.detach().cpu().item()))\n",
        "\n",
        "        mean_loss = float(np.mean(losses))\n",
        "        epoch_losses.append(mean_loss)\n",
        "        print(f\"Epoch {ep:02d}/{epochs} | loss={mean_loss:.6f}\")\n",
        "\n",
        "    return epoch_losses\n",
        "\n",
        "\n",
        "def reconstruction_scores(model: nn.Module, x: np.ndarray, batch_size: int = 128) -> np.ndarray:\n",
        "    \"\"\"Compute per-window reconstruction error scores.\"\"\"\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(torch.tensor(x, dtype=torch.float32))\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    scores: List[float] = []\n",
        "    loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (xb,) in loader:\n",
        "            xb = xb.to(device)\n",
        "            recon = model(xb)\n",
        "\n",
        "            per_elem = loss_fn(recon, xb)\n",
        "            per_window = per_elem.mean(dim=(1, 2)).detach().cpu().numpy()\n",
        "            scores.extend(per_window.tolist())\n",
        "\n",
        "    return np.array(scores, dtype=float)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5078b8fb",
      "metadata": {
        "id": "5078b8fb"
      },
      "source": [
        "### 8.1 Train + score with BiLSTM AE\n",
        "\n",
        "I fit on the “mostly normal” early segment, then score the later segment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cab54c5a",
      "metadata": {
        "id": "cab54c5a"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "train_scaled = scaler.fit_transform(train_df)\n",
        "test_scaled = scaler.transform(test_df)\n",
        "\n",
        "cfg = SequenceConfig(window=32, stride=1)\n",
        "\n",
        "train_w = make_windows(train_scaled, cfg)\n",
        "test_w = make_windows(test_scaled, cfg)\n",
        "\n",
        "print(\"Train windows:\", train_w.shape, \"| Test windows:\", test_w.shape)\n",
        "\n",
        "model = BiLSTMAutoencoder(n_features=train_w.shape[-1], hidden=32)\n",
        "losses = train_autoencoder(model, train_w, epochs=8, batch_size=64, lr=1e-3)\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(losses)\n",
        "plt.title(\"BiLSTM AE training loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "train_scores = reconstruction_scores(model, train_w)\n",
        "test_scores = reconstruction_scores(model, test_w)\n",
        "\n",
        "THRESH_PCTL = 90\n",
        "threshold = float(np.percentile(train_scores, THRESH_PCTL))\n",
        "print(f\"Threshold (p{THRESH_PCTL}) = {threshold:.6f}\")\n",
        "\n",
        "test_flags = (test_scores >= threshold).astype(int)\n",
        "print(\"Flagged anomalies in test windows:\", int(test_flags.sum()), \"out of\", len(test_flags))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc41e27",
      "metadata": {
        "id": "0dc41e27"
      },
      "source": [
        "### 8.2 Align window scores back to timestamps\n",
        "\n",
        "Window scores start after `window-1` steps, so I align them to the end of each window.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "629f6283",
      "metadata": {
        "id": "629f6283"
      },
      "outputs": [],
      "source": [
        "test_index = test_df.index[cfg.window - 1 :]\n",
        "bilstm_df = pd.DataFrame(\n",
        "    {\"recon_score\": test_scores, \"anomaly\": test_flags},\n",
        "    index=test_index,\n",
        ")\n",
        "\n",
        "plot_series(\n",
        "    bilstm_df.join(test_df[[\"rms\"]].iloc[cfg.window - 1 :]),\n",
        "    [\"rms\", \"recon_score\"],\n",
        "    title=\"BiLSTM AE - RMS vs reconstruction score\",\n",
        "    fname=\"bilstm_score_vs_rms\",\n",
        ")\n",
        "\n",
        "print(\"Sample flagged timestamps:\")\n",
        "bilstm_df[bilstm_df[\"anomaly\"] == 1].head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c20186c",
      "metadata": {
        "id": "5c20186c"
      },
      "source": [
        "## 9. Next steps\n",
        "\n",
        "- Add more channels and richer features (spectral bands, envelope, crest factor)\n",
        "- Use a time-aware evaluation protocol (lead time, early-warning horizon)\n",
        "- Persist the scaler + AE weights under `artifacts/` for deployment\n",
        "\n",
        "For GitHub, I recommend adding a `requirements.txt` and a short `README.md` that cites the dataset source.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}